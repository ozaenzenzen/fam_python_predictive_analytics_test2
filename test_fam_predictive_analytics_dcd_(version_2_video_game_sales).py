# -*- coding: utf-8 -*-
"""test_fam_predictive_analytics_dcd (version 2 - video game sales).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PWmoJbGZEwI4ZrbZtzF70O4-r4l2XtY-

#Import Library
Proses untuk menginisialisasi library yang akan dibutuhkan untuk proses mengolah data untuk Predictive Analysis

Mengimpor library google drive untuk mengambil dataset yang disimpan dalam Google Drive
"""

from google.colab import drive
drive.mount('/content/drive')

"""Mengimpor library yang dibutuhkan untuk proses predictive analysis"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns

"""#Data Loading
Tahap untuk memuat dataset ke dalam variabel dalam python

Menuliskan url atau direktori dari tempat dataset disimpan
"""

url = "/content/drive/MyDrive/File Ozan/Repo Machine Learning/vgsales.csv"

"""Memanggil dataset berformat .csv ke dalam variabel dataset menggunakan library pandas"""

dataset_init = pd.read_csv(url)
dataset_init

"""#Data Preparation
Tahap untuk mengolah data agar data siap digunakan untuk proses modeling. Tahapan yang dilakukan antara lain
- `Data Info`: Tahap untuk memuat informasi dari dataset
- `Check Missing Value` : Melakukan pengecekan missing value dalam data (bisa NULL atau 0) menyesuaikan analisis masalah
- `Imputation Missing Value` : Solusi untuk mengganti suatu missing value dengan suatu nilai pengganti yang didapatkan dari teknik khusus tertentu
- `Outlier Analysis` : Tahap untuk melakukan analisis & memvisualisasikan persebaran data pada setiap kolom untuk mengetahui outlier = sebuah data atau observasi yang menyimpang secara ekstrim dari rata-rata sekumpulan data yang ada
- `Drop Outlier` : Tahap untuk menangani outlier pada dataset. Disini digunakan metode Inter Quartile Range (IQR)
- `Univariate Analysis` : Pada tahap ini merupakan proses untuk mengeksplorasi dan menjelaskan setiap variabel dalam kumpulan data secara terpisah untuk 1 jenis variabel / kolom
- `Multivariate Analysis` : Pada tahap ini merupakan proses untuk mengeksplorasi dan menjelaskan setiap variabel dalam kumpulan data secara terpisah untuk 2 atau lebih jenis variabel / kolom
- `Drop Column` : Tahap untuk melakukan penghilangan kolom yang tidak diperlukan sesuai analisis masalah dan tujuan penelitian
- `Encoding Categorical Features` : Tahap untuk memberikan alias dalam bentuk numerik kepada kolom yang bersifat kategorikal
- `Train Test Split` : Tahap untuk membagi dataset menjadi data train dan data test dalam pembagian yang ditentukan
- `Standarisasi` : Tahap untuk melakukan perubahan skala nilai pada suatu kolom sesuai skala yang diinginkan

"""

dataset = dataset_init

"""###Data Info
Tahap untuk memuat informasi dari dataset

Melihat informasi mengenai dataset antara lain
- Jumlah dan nama kolom
- Jumlah baris dari setiap kolom
- Tipe nullable atau tidak dari tiap kolom
- Tipe data dari tiap kolom
"""

dataset.info()

"""`dataset.describe()` Ditujukan untuk mendeskripsikan beberapa parameter untuk seluruh kolom dari dataset. Parameter tersebut antara lain
- Count atau jumlah baris dari setiap kolom
- Mean atau rata-rata dari setiap kolom
- Standar deviasi dari setiap kolom
- Nilai minimum / terkecil dari setiap kolom
- Nilai kuartil pertama atau 25% dari setiap kolom
- Nilai kuartil kedua atau 50% atau median dari setiap kolom
- Nilai kuartil ketiga atau 75% dari setiap kolom
- Nilai maximum / terbesar dari setiap kolom
"""

dataset.describe()

"""`dataset.columns` Ditujukan untuk mejabarkan kolom apa saja yang tersedia pada dataset yang digunakan"""

dataset.columns

"""###Check Missing Value
Melakukan pengecekan missing value dalam data (bisa NULL atau 0) menyesuaikan analisis masalah.

Hasil:
- didapatkan missing value pada kolom `Year` yang bersifat numerik
- didapatkan missing value pada kolom `Publisher` yang bersifat kategorik

Tahap untuk melakukan pengecekan NULL pada setiap kolom, didapatkan hasil tidak ada NULL pada setiap kolom
"""

# dataset.isnull().sum()*100/dataset.shape[0]
dataset.isnull().sum()

"""###Imputation Missing Value
Solusi untuk mengganti suatu missing value dengan suatu nilai pengganti yang didapatkan dari teknik khusus tertentu.

Hasil: Pada tahap ini digunakan
- teknik imputasi berdasarkan nilai rata-rata (mean) untuk kolom `Year`
- teknik imputasi berdasarkan nilai terbanyak (most frequent) untuk kolom `Publisher`
"""

from sklearn.impute import SimpleImputer
numeric_transformer = SimpleImputer(strategy='mean')  # Use mean for numerical features
categorical_transformer = SimpleImputer(strategy='most_frequent')  # Use mode for categorical features

dataset['Year'] = numeric_transformer.fit_transform(dataset[['Year']])
dataset['Publisher'] = categorical_transformer.fit_transform(dataset[['Publisher']])

# Cek ukuran data untuk memastikan baris sudah di-drop
dataset.shape

dataset.isnull().sum()

"""###Outlier Analysis
Tahap untuk melakukan analisis & memvisualisasikan persebaran data pada setiap kolom untuk mengetahui outlier = sebuah data atau observasi yang menyimpang secara ekstrim dari rata-rata sekumpulan data yang ada

Hasil: Didapatkan outlier pada kolom `Year`, `NA_Sales`, `EU_Sales`, `JP_Sales`, dan `Other_Sales`

Tahap untuk melakukan analisis & memvisualisasikan persebaran data pada setiap kolom. Pada baris kode ini dilakukan penghilangan kolom yang bersifat kategorial yaitu `Name`, `Genre`, `Publisher`, dan `Platform`
"""

# num_dataset = dataset
num_dataset = dataset.drop(columns=['Name', 'Genre', 'Publisher', 'Platform'])
num_dataset

for column in num_dataset:
    plt.figure()
    num_dataset.boxplot([column])

"""Tahap yang sama dengan sebelumnya, yaitu untuk melakukan analisis & memvisualisasikan persebaran data pada setiap kolom. Hanya saja menggunakan visualisasi yang berbeda

######Check Outlier With Z-Score

Tahap untuk melakukan analisis & meggambarkan persebaran data pada setiap kolom menggunakan metode Z-Score
"""

#Check with z-score
import scipy.stats as stats
stats.zscore(num_dataset)

"""###Drop Outliers
Tahap untuk menangani outlier pada dataset. Disini digunakan metode Inter Quartile Range (IQR).

Tahap untuk menangani outlier pada dataset. Disini digunakan metode Inter Quartile Range (IQR). Didapatkan data hasil drop yaitu 7677 baris dan 12 kolom
"""

Q1 = dataset.quantile(0.25)
Q3 = dataset.quantile(0.75)
IQR=Q3-Q1
dataset=dataset[~((dataset<(Q1-1.5*IQR))|(dataset>(Q3+1.5*IQR))).any(axis=1)]

# Cek ukuran dataset setelah kita drop outliers
dataset.shape

"""###Univariate Analysis
Pada tahap ini merupakan proses untuk mengeksplorasi dan menjelaskan setiap variabel dalam kumpulan data secara terpisah untuk 1 jenis variabel / kolom

Pada tahap baris kode ini ditujukan untuk membagi kolom numerik dan kategorikal ke dalam masing-masing variabel
"""

numerical_features = ['Rank', 'Year', 'NA_Sales',
       'EU_Sales', 'JP_Sales', 'Other_Sales', 'Global_Sales']
categorical_features = ['Name', 'Genre', 'Publisher', 'Platform']

"""######Categorical Features

Pada tahap baris kode ini ditujukan mendeskripsikan jumlah sampel dan persentase kolom kategorik dari `Genre`
"""

#feature Genre
feature = categorical_features[1]
count = dataset[feature].value_counts()
percent = 100*dataset[feature].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title=feature);

"""Pada tahap baris kode ini ditujukan mendeskripsikan jumlah sampel dan persentase kolom kategorik dari `Pubsliher`"""

#feature Pubsliher
feature = categorical_features[2]
count = dataset[feature].value_counts()
percent = 100*dataset[feature].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title=feature);

"""Pada tahap baris kode ini ditujukan mendeskripsikan jumlah sampel dan persentase kolom kategorik dari `Platform`"""

#feature Platform
feature = categorical_features[3]
count = dataset[feature].value_counts()
percent = 100*dataset[feature].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title=feature);

"""######Numerical Features

Pada tahap baris kode ini ditujukan visualisai persebaran dari kolom`'Rank', 'Year', 'NA_Sales',
       'EU_Sales', 'JP_Sales', 'Other_Sales', 'Global_Sales'`
"""

dataset.hist(bins=50, figsize=(20,15))
plt.show()

"""###Multivariate Analysis
Pada tahap ini merupakan proses untuk mengeksplorasi dan menjelaskan setiap variabel dalam kumpulan data secara terpisah untuk 2 atau lebih jenis variabel / kolom

######Categorical Features

Visualisasi untuk rata-rata kolom `Year` terhadap kolom kategorik `'Genre'`, `'Publisher'`, dan `'Platform'`
"""

cat_features = dataset.select_dtypes(include='object').columns.difference(['Name']).to_list()

for col in cat_features:
  sns.catplot(x=col, y="Year", kind="bar", dodge=False, height = 4, aspect = 3,  data=dataset, palette="Set3")
  plt.title("Rata-rata 'Year' Relatif terhadap - {}".format(col))

cat_features

"""Visualisasi untuk rata-rata kolom `EU_Sales` terhadap kolom kategorik `'Genre'`, `'Publisher'`, dan `'Platform'`"""

cat_features = dataset.select_dtypes(include='object').columns.difference(['Name']).to_list()

for col in cat_features:
  sns.catplot(x=col, y="EU_Sales", kind="bar", dodge=False, height = 4, aspect = 3,  data=dataset, palette="Set3")
  plt.title("Rata-rata 'EU_Sales' Relatif terhadap - {}".format(col))

cat_features

"""Visualisasi untuk rata-rata kolom `JP_Sales` terhadap kolom kategorik `'Genre'`, `'Publisher'`, dan `'Platform'`"""

cat_features = dataset.select_dtypes(include='object').columns.difference(['Name']).to_list()

for col in cat_features:
  sns.catplot(x=col, y="JP_Sales", kind="bar", dodge=False, height = 4, aspect = 3,  data=dataset, palette="Set3")
  plt.title("Rata-rata 'JP_Sales' Relatif terhadap - {}".format(col))

cat_features

"""Visualisasi untuk rata-rata kolom `Other_Sales` terhadap kolom kategorik `'Genre'`, `'Publisher'`, dan `'Platform'`"""

cat_features = dataset.select_dtypes(include='object').columns.difference(['Name']).to_list()

for col in cat_features:
  sns.catplot(x=col, y="Other_Sales", kind="bar", dodge=False, height = 4, aspect = 3,  data=dataset, palette="Set3")
  plt.title("Rata-rata 'Other_Sales' Relatif terhadap - {}".format(col))

cat_features

"""######Numerical Features

Visualisasi keterkaitan data untuk setiap kolom numerik dengan kolom numerik lainnya. Disini juga untuk melihat korelasi dengan kolom target
"""

# Mengamati hubungan antar fitur numerik dengan fungsi pairplot()
sns.pairplot(dataset, diag_kind = 'kde')

"""Visualisasi matriks korelasi data untuk setiap kolom numerik dengan kolom numerik lainnya"""

plt.figure(figsize=(10, 8))
correlation_matrix = dataset.corr().round(2)

# Untuk menge-print nilai di dalam kotak, gunakan parameter anot=True
sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, )
plt.title("Correlation Matrix untuk Fitur Numerik ", size=22)

"""###Drop Column
Tahap untuk melakukan penghilangan kolom yang tidak diperlukan sesuai analisis masalah dan tujuan penelitian.

Hasil: Penghilangan kolom `Rank` ditujukan untuk mengurangi jumlah kolom berkorelasi sangat jauh secara domain knowledge dari dataset, alasan penghilangan kolom `Rank` karena kolom tersebut merupakan sebuah identifier dari dataset.
"""

dataset.drop(['Rank'], inplace=True, axis=1)

dataset.head()

"""### Encoding Categorical Features
Tahap untuk memberikan alias dalam bentuk numerik kepada kolom yang bersifat kategorikal

Hasil: Kolom kategorikal ada 2 yaitu `Name`, `Genre`, `Publisher`, dan `Platform`. Maka proses yang dilakukan adalah
- melakukan encoding terhdapat kolom tersebut, kecuali kolom `Name` yang hanya sebuah identifier
- memisahkan hasil encode menjadi masing-masing kolom
- menghapus kolom `Name`, `Genre`, `Publisher`, dan `Platform`
"""

from sklearn.preprocessing import OneHotEncoder
dataset = pd.concat([dataset, pd.get_dummies(dataset['Genre'], prefix='country')],axis=1)
dataset = pd.concat([dataset, pd.get_dummies(dataset['Publisher'], prefix='gender')],axis=1)
dataset = pd.concat([dataset, pd.get_dummies(dataset['Platform'], prefix='gender')],axis=1)
dataset.drop(['Name', 'Genre', 'Publisher', 'Platform'], axis=1, inplace=True)
# dataset.drop(['country','gender','credit_card','active_member','churn'], axis=1, inplace=True)
# dataset.head()
dataset

"""###Train Test Split
Tahap untuk membagi dataset menjadi data train dan data test dalam pembagian yang ditentukan
"""

from sklearn.model_selection import train_test_split

X = dataset.drop(["Global_Sales"],axis =1)
y = dataset["Global_Sales"]

# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 123)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 123)

X_train

y_train

X_test

y_test

print(f'Total # of sample in whole dataset: {len(X)}')
print(f'Total # of sample in train dataset: {len(X_train)}')
print(f'Total # of sample in test dataset: {len(X_test)}')

"""###Standarisasi
Tahap untuk melakukan perubahan skala nilai pada kolom `'Year', 'NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales'` sesuai skala yang diinginkan
"""

from sklearn.preprocessing import StandardScaler

numerical_features = ['Year', 'NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales']
scaler = StandardScaler()
scaler.fit(X_train[numerical_features])
X_train[numerical_features] = scaler.transform(X_train.loc[:, numerical_features])
X_train[numerical_features].head()

"""#Modelling
Tahap ini merupakan proses untuk memodelkan data menggunakan algoritma yang disesuaikan dengan analisis masalah penelitian.
Pada tahap ini menggunakan algoritma regression
- KNN
- Random Forest
- Boosting Algorithm
- Support Vector Regression

###Using KNN
"""

# Siapkan dataframe untuk analisis model
models = pd.DataFrame(index=['train_mse', 'test_mse'],
                      columns=['KNN', 'RandomForest', 'Boosting', 'SVR'])

from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error

knn = KNeighborsRegressor(n_neighbors=10)
knn.fit(X_train, y_train)

y_pred_knn = knn.predict(X_train)

models.loc['train_mse','knn'] = mean_squared_error(y_pred = y_pred_knn, y_true=y_train)

"""###Using Random Forest"""

from sklearn.ensemble import RandomForestRegressor

RF = RandomForestRegressor(n_estimators=50, max_depth=16, random_state=55, n_jobs=-1)
RF.fit(X_train, y_train)

y_pred_rf = RF.predict(X_train)

models.loc['train_mse','RandomForest'] = mean_squared_error(y_pred=y_pred_rf, y_true=y_train)

"""###Using Boosting Algorithm"""

from sklearn.ensemble import AdaBoostRegressor

boosting = AdaBoostRegressor(learning_rate=0.005, random_state=55)
boosting.fit(X_train, y_train)

y_pred_b = boosting.predict(X_train)

models.loc['train_mse','Boosting'] = mean_squared_error(y_pred=y_pred_b, y_true=y_train)

"""###Using SVR"""

from sklearn.svm import SVR

svr = SVR()
svr.fit(X_train, y_train)

y_pred_lr = svr.predict(X_train)

models.loc['train_mse','SVR'] = mean_squared_error(y_pred=y_pred_lr, y_true=y_train)

"""#Evaluation
Tahap ini merupakan proses melihat dan menguji kualitas model yang telah dibuat menggunakan data train dan data test

###Scaling data X_test
"""

# Lakukan scaling terhadap fitur numerik pada X_test sehingga memiliki rata-rata=0 dan varians=1
X_test.loc[:, numerical_features] = scaler.transform(X_test[numerical_features])
X_test.loc[:, numerical_features].head()

X_test

"""###Accuracy Eval / R Squared Score (Using Data Test)"""

acc = pd.DataFrame(index=['accuracy'])

acc.loc['accuracy', 'knn'] = knn.score(X_test, y_test)
knn.score(X_test, y_test)

acc.loc['accuracy', 'random_forest'] = RF.score(X_test, y_test)
RF.score(X_test, y_test)

acc.loc['accuracy', 'adaboost'] = boosting.score(X_test, y_test)
boosting.score(X_test, y_test)

acc.loc['accuracy', 'SVR'] = svr.score(X_test, y_test)
svr.score(X_test, y_test)

acc

"""###Mean Squared Error Eval"""

# Buat variabel mse yang isinya adalah dataframe nilai mse data train dan test pada masing-masing algoritma
mse = pd.DataFrame(columns=['train', 'test'], index=['KNN','RF','Boosting', 'SVR'])

# Buat dictionary untuk setiap algoritma yang digunakan
model_dict = {'KNN': knn, 'RF': RF, 'Boosting': boosting, 'SVR':svr}

# Hitung Mean Squared Error masing-masing algoritma pada data train dan test
for name, model in model_dict.items():
    mse.loc[name, 'train'] = mean_squared_error(y_true=y_train, y_pred=model.predict(X_train))/1e3
    mse.loc[name, 'test'] = mean_squared_error(y_true=y_test, y_pred=model.predict(X_test))/1e3

# Panggil mse
mse

fig, ax = plt.subplots()
mse.sort_values(by='test', ascending=False).plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)

"""###Predict Using X_test"""

prediksi = X_test.iloc[:1000].copy()
pred_dict = {'y_true':y_test[:1000]}
for name, model in model_dict.items():
    pred_dict['prediksi_'+name] = model.predict(prediksi).round(1)

pred_use_data_test = pd.DataFrame(pred_dict)
pred_use_data_test

"""###R Squared Score Eval Using Predicted Data"""

prediksi_r2 = pred_use_data_test

from sklearn.metrics import r2_score

pred_dict_r2 = pd.DataFrame(index=['r2_score'])
for name, model in model_dict.items():
    # pred_dict_r2.loc['r2_score', name] = r2_score(prediksi_r2['prediksi_'+name], prediksi_r2['y_true'])
    pred_dict_r2.loc['r2_score', name] = r2_score(prediksi_r2['y_true'], prediksi_r2['prediksi_'+name])

pred_dict_r2